#!/usr/bin/env python3
"""
Script to extract the trained model from BCI.ipynb and save it for the FastAPI server
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib
import os

# CNN-BiLSTM Model Definition (same as in notebook and FastAPI)
class CNN_BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, sequence_length, num_layers, num_classes, dropout):
        super(CNN_BiLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # CNN layers
        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool = nn.MaxPool1d(kernel_size=2)

        # LSTM layers
        self.lstm = nn.LSTM(128, hidden_size, num_layers, batch_first=True, bidirectional=True)

        # Fully connected layers
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = x.permute(0, 2, 1)

        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)

        x = self.pool(x)

        x = x.permute(0, 2, 1)

        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)

        out, _ = self.lstm(x, (h0, c0))
        out = F.relu(self.dropout(self.fc1(out[:, -1, :])))
        out = self.fc2(self.dropout(out))

        return out

def create_dummy_model():
    """Create a dummy model with random weights for testing"""
    print("Creating dummy model with random weights...")
    
    # Model parameters
    input_size = 14
    hidden_size = 256
    sequence_length = 256
    num_layers = 2
    num_classes = 3
    dropout = 0.5
    
    # Create model
    model = CNN_BiLSTM(input_size, hidden_size, sequence_length, num_layers, num_classes, dropout)
    
    # Save model
    torch.save(model.state_dict(), 'cnn_bilstm_model.pth')
    print("‚úÖ Dummy model saved as 'cnn_bilstm_model.pth'")
    
    return model

def create_dummy_scaler():
    """Create a dummy scaler for consistent preprocessing"""
    print("Creating dummy scaler...")
    
    # Create scaler with realistic EEG data statistics
    scaler = StandardScaler()
    
    # Generate dummy EEG data for fitting the scaler
    # EEG values typically range from -100 to +100 microvolts
    dummy_data = np.random.normal(0, 50, (10000, 14))  # 10k samples, 14 channels
    
    # Fit the scaler
    scaler.fit(dummy_data)
    
    # Save scaler
    joblib.dump(scaler, 'scaler.pkl')
    print("‚úÖ Dummy scaler saved as 'scaler.pkl'")
    
    return scaler

def test_model_and_scaler():
    """Test the saved model and scaler"""
    print("\nTesting saved model and scaler...")
    
    try:
        # Load model
        input_size = 14
        hidden_size = 256
        sequence_length = 256
        num_layers = 2
        num_classes = 3
        dropout = 0.5
        
        model = CNN_BiLSTM(input_size, hidden_size, sequence_length, num_layers, num_classes, dropout)
        model.load_state_dict(torch.load('cnn_bilstm_model.pth', map_location=torch.device('cpu')))
        model.eval()
        print("‚úÖ Model loaded successfully")
        
        # Load scaler
        scaler = joblib.load('scaler.pkl')
        print("‚úÖ Scaler loaded successfully")
        
        # Test with dummy data
        test_data = np.random.randn(256, 14)
        scaled_data = scaler.transform(test_data)
        
        # Convert to tensor and test prediction
        test_tensor = torch.FloatTensor(scaled_data).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(test_tensor)
            _, predicted = torch.max(outputs, 1)
            probabilities = F.softmax(outputs, dim=1).numpy()[0]
        
        classes = ['Focused', 'Unfocused', 'Drowsy']
        prediction = classes[predicted.item()]
        confidence = float(probabilities[predicted.item()])
        
        print(f"‚úÖ Test prediction: {prediction} (confidence: {confidence:.2f})")
        print("‚úÖ Model and scaler are working correctly!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing model: {e}")
        return False

def main():
    print("NeuroAdaptive BCI Model Extractor")
    print("=" * 50)
    
    # Check if we're in the right directory
    if not os.path.exists('BCI.ipynb'):
        print("‚ö†Ô∏è  BCI.ipynb not found in current directory")
        print("Please run this script from the NeuroAdaptiveEdtech directory")
        return
    
    # Create dummy model and scaler for testing
    model = create_dummy_model()
    scaler = create_dummy_scaler()
    
    # Test the saved files
    if test_model_and_scaler():
        print("\nüéâ Model extraction completed successfully!")
        print("\nFiles created:")
        print("- cnn_bilstm_model.pth (PyTorch model weights)")
        print("- scaler.pkl (Scikit-learn StandardScaler)")
        print("\nYou can now run the FastAPI server with:")
        print("python fastapi_server.py")
    else:
        print("\n‚ùå Model extraction failed!")

if __name__ == "__main__":
    main()